{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "print(mnist)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 250\n",
    "batch_size = 50  \n",
    "display_step = 1\n",
    "n_input = 784  \n",
    "n_classes = 10  \n",
    "\n",
    "n_hidden_layer1 = 256 # layer number of features\n",
    "n_hidden_layer2 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Bias\n",
    "weights = {\n",
    "    'hidden_layer_1': tf.Variable(tf.random_normal([n_input, n_hidden_layer1])),\n",
    "    'hidden_layer_2': tf.Variable(tf.random_normal([n_input, n_hidden_layer2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer1': tf.Variable(tf.random_normal([n_hidden_layer1])),\n",
    "    'hidden_layer2': tf.Variable(tf.random_normal([n_hidden_layer2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Flow Graph\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer-1 with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer_1']), biases['hidden_layer1'])\n",
    "layer_1 = tf.nn.relu(layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layer-2 with RELU activation\n",
    "layer_2 = tf.add(tf.matmul(x_flat, weights['hidden_layer_2']), biases['hidden_layer2'])\n",
    "layer_2 = tf.nn.relu(layer_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer with linear activation\n",
    "logits = tf.matmul(layer_2, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\",                 \"{:.9f}\".format(c))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    #print(a)\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "    #print(confusion_matrix(np.argmax(pred),np.argmax(mnist.test.labels)))\n",
    "    \n",
    "    confusionmatrix = tf.contrib.metrics.confusion_matrix(tf.argmax(y,1),tf.argmax(pred,1))\n",
    "    print('Confusion Matrix: \\n\\n', tf.Tensor.eval(confusionmatrix,feed_dict={x: mnist.test.images, y: mnist.test.labels}, session=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the graph\n",
    "X = ([50, 125, 250, 500])\n",
    "Y_50 = ([0.9125, 0.9297, 0.933, 0.9368])\n",
    "Y_125 = ([0.9057, 0.9143, 0.9245, 0.9301])\n",
    "Y_250 = ([0.8711, 0.8927, 0.9155, 0.9188])\n",
    "Y_500 = ([0.8459, 0.8756, 0.8962, 0.9083])\n",
    "\n",
    "plt.plot(X,Y_50,'b--', label = \"BatchSize=50\")\n",
    "plt.plot(X,Y_125,'r--', label = \"BatchSize=125\")\n",
    "plt.plot(X,Y_250,'g--', label = \"BatchSize=250\")\n",
    "plt.plot(X,Y_500,'y--', label = \"BatchSize=500\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a function of Epoch_Two Layer_NN')\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
